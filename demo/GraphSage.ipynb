{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "employed-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secondary-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        # block 是我们采样获得的二部图，这里用于消息传播\n",
    "        # x 为节点特征\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            h_dst = h[:block.number_of_dst_nodes()]\n",
    "            h = layer(block, (h, h_dst))\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, g, x, batch_size, device):\n",
    "        # inference 用于评估测试，针对的是完全图\n",
    "        # 目前会出现重复计算的问题，优化方案还在 to do list 上\n",
    "        nodes = th.arange(g.number_of_nodes())\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            y = th.zeros(g.number_of_nodes(),\n",
    "                         self.n_hidden if l != len(self.layers) - 1 else self.n_classes)\n",
    "            for start in trange(0, len(nodes), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_nodes = nodes[start:end]\n",
    "                block = dgl.to_block(dgl.in_subgraph(g, batch_nodes), batch_nodes)\n",
    "                input_nodes = block.srcdata[dgl.NID]\n",
    "                h = th.Tensor(x[input_nodes]).to(device)\n",
    "                h_dst = h[:block.number_of_dst_nodes()]\n",
    "                h = layer(block, (h, h_dst))\n",
    "                if l != len(self.layers) - 1:\n",
    "                    h = self.activation(h)\n",
    "                    h = self.dropout(h)\n",
    "                y[start:end] = h.cpu()\n",
    "            x = y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            g.edata['score'] = F.sigmoid(g.edata['score'])\n",
    "            return g.edata['score'][:, 0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "front-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborSampler(object):\n",
    "    def __init__(self, g, fanouts):\n",
    "        \"\"\"\n",
    "        g 为 DGLGraph；\n",
    "        fanouts 为采样节点的数量，实验使用 10,25，指一阶邻居采样 10 个，二阶邻居采样 25 个。\n",
    "        \"\"\"\n",
    "        self.g = g\n",
    "        self.fanouts = fanouts\n",
    "\n",
    "    def sample_blocks(self, seeds):\n",
    "        seeds = th.LongTensor(np.asarray(seeds))\n",
    "        blocks = []\n",
    "        for fanout in self.fanouts: \n",
    "            # sample_neighbors 可以对每一个种子的节点进行邻居采样并返回相应的子图\n",
    "            # replace=True 表示用采样后的邻居节点代替所有邻居节点\n",
    "            frontier = dgl.sampling.sample_neighbors(g, seeds, fanout, replace=True)\n",
    "            # 将图转变为可以用于消息传递的二部图（源节点和目的节点）\n",
    "            # 其中源节点的 id 也可能包含目的节点的 id（原因上面说了）\n",
    "            # 转变为二部图主要是为了方便进行消息传递\n",
    "            block = dgl.to_block(frontier, seeds)\n",
    "            # 获取新图的源节点作为种子节点，为下一层作准备\n",
    "            # 之所以是从 src 中获取种子节点，是因为采样操作相对于聚合操作来说是一个逆向操作\n",
    "            seeds = block.srcdata[dgl.NID]\n",
    "            # 把这一层放在最前面。\n",
    "            # PS：如果数据量大的话，插入操作是不是不太友好。\n",
    "            blocks.insert(0, block)\n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "engaging-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(pred, labels): # 计算准确率\n",
    "    y_pred = th.argmax(pred, dim=1) # 按行取argmax,得到预测的标签\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(labels, y_pred).ravel() # y_true, y_pred\n",
    "    \n",
    "    accuracy = (tn+tp)/len(labels)\n",
    "    pos_acc = tp/sum(labels).item()\n",
    "    neg_acc = tn/(len(y_pred)-sum(y_pred).item()) # [y_true=0 & y_pred=0] / y_pred=0\n",
    "    \n",
    "    neg_recall = tn / (tn+fp) # [y_true=0 & y_pred=0] / y_true=0\n",
    "    return neg_recall, neg_acc, pos_acc, accuracy\n",
    "\n",
    "def evaluate(model, g, inputs, labels, val_mask, batch_size, device):\n",
    "    \"\"\"\n",
    "    评估模型，调用 model 的 inference 函数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        pred = model.inference(g, inputs, batch_size, device)\n",
    "    model.train()\n",
    "    return compute_acc(pred[val_mask], labels[val_mask])\n",
    "\n",
    "def load_subtensor(g, labels, seeds, input_nodes, device):\n",
    "    \"\"\"\n",
    "    将一组节点的特征和标签复制到 GPU 上。\n",
    "    \"\"\"\n",
    "    batch_inputs = th.Tensor(g.ndata['features'][input_nodes]).to(device)\n",
    "    batch_labels = labels[seeds].to(device)\n",
    "    return batch_inputs, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mechanical-coast",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-00478730654a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_mask_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature' is not defined"
     ]
    }
   ],
   "source": [
    "features = th.FloatTensor(feature)\n",
    "labels = th.LongTensor(labels)\n",
    "\n",
    "train_mask = deepcopy(split_mask_idx)\n",
    "train_mask[train_idx] = 1\n",
    "train_mask[test_idx] = 0\n",
    "train_mask = th.BoolTensor(train_mask)\n",
    "\n",
    "test_mask = deepcopy(split_mask_idx)\n",
    "test_mask[train_idx] = 0\n",
    "test_mask[test_idx] = 1\n",
    "test_mask = th.BoolTensor(test_mask)\n",
    "\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "in_feats = feature.shape[1] # 输入维度\n",
    "n_classes = 2 # label的种类数\n",
    "\n",
    "gpu = -1\n",
    "num_epochs = 50\n",
    "num_hidden = 64\n",
    "num_layers = 2\n",
    "fan_out = '5,5'\n",
    "batch_size = 1024\n",
    "log_every = 20  # 记录日志的频率\n",
    "eval_every = 2\n",
    "lr = 0.001\n",
    "dropout = 0\n",
    "num_workers = 0  # 用于采样进程的数量\n",
    "\n",
    "if gpu >= 0:\n",
    "    device = th.device('cuda:%d' % gpu)\n",
    "else:\n",
    "    device = th.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "threaded-practitioner",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b1f3f6bfd87f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create PyTorch DataLoader for constructing blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# collate_fn 参数指定了 sampler，可以对 batch 中的节点进行采样\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeighborSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfanout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfan_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m dataloader = DataLoader(\n\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "gcn_msg = fn.copy_src(src = 'h', out = 'm')\n",
    "gcn_reduce = fn.sum(msg = 'm', out = 'h')\n",
    "\n",
    "# Create PyTorch DataLoader for constructing blocks\n",
    "# collate_fn 参数指定了 sampler，可以对 batch 中的节点进行采样\n",
    "sampler = NeighborSampler(g, [int(fanout) for fanout in fan_out.split(',')])\n",
    "dataloader = DataLoader(\n",
    "    dataset = train_idx,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = sampler.sample_blocks,\n",
    "    shuffle = True,\n",
    "    drop_last = False,\n",
    "    num_workers = num_workers)\n",
    "\n",
    "model = GraphSAGE(in_feats, num_hidden, n_classes, num_layers, F.relu, dropout)\n",
    "model = model.to(device)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "loss_fcn = loss_fcn.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "avg = 0\n",
    "iter_tput = []\n",
    "for epoch in range(num_epochs):\n",
    "    tic = time.time()\n",
    "\n",
    "    for step, blocks in enumerate(dataloader):\n",
    "        tic_step = time.time()\n",
    "\n",
    "        input_nodes = blocks[0].srcdata[dgl.NID]\n",
    "        seeds = blocks[-1].dstdata[dgl.NID]\n",
    "\n",
    "        # Load the input features as well as output labels\n",
    "        batch_inputs, batch_labels = load_subtensor(g, labels, seeds, input_nodes, device)\n",
    "\n",
    "        # Compute loss and prediction\n",
    "        batch_pred = model(blocks, batch_inputs)\n",
    "        loss = loss_fcn(batch_pred, batch_labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_tput.append(len(seeds) / (time.time() - tic_step))\n",
    "        if step % log_every == 0:\n",
    "            train_neg_recall, train_neg_acc, train_pos_acc, train_accuracy = compute_acc(batch_pred, batch_labels)\n",
    "            gpu_mem_alloc = th.cuda.max_memory_allocated() / 1000000 if th.cuda.is_available() else 0\n",
    "            print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MiB'.format(\n",
    "                epoch, step, loss.item(), np.mean(iter_tput[3:]), gpu_mem_alloc))\n",
    "            print('Train: Neg Recall = {:.2f} | Neg Acc = {:.2f} | Pos Acc = {:.2f} | All Acc = {:.2f}'.format(train_neg_recall, train_neg_acc, train_pos_acc, train_accuracy))\n",
    "\n",
    "    toc = time.time()\n",
    "    print('Epoch Time(s): {:.4f}'.format(toc - tic))\n",
    "    if epoch >= 5:\n",
    "        avg += toc - tic\n",
    "    if epoch % eval_every == 0 and epoch != 0:\n",
    "        test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy = evaluate(model, g, g.ndata['features'], labels, test_mask, batch_size, device)\n",
    "        print('Test: Neg Recall = {:.2f} | Neg Acc = {:.2f} | Pos Acc = {:.2f} | All Acc = {:.2f}'.format(test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy))\n",
    "\n",
    "test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy = evaluate(model, g, g.ndata['features'], labels, test_mask, batch_size, device)\n",
    "print('Test: Neg Recall = {:.2f} | Neg Acc = {:.2f} | Pos Acc = {:.2f} | All Acc = {:.2f}'.format(test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy))\n",
    "\n",
    "print('Avg epoch time: {}'.format(avg / epoch))http://phabricator.ushow.media/D157086\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
