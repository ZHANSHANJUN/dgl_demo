{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建图\n",
    " \n",
    "# edge_data是vroleid, friend_level, friend_roleid\n",
    "# u,v=src,dst; eid是edge_data中的第几行\n",
    "edge_norm = np.zeros(edge_data.shape[0])\n",
    "for e in tqdm(range(1, 150)):\n",
    " \n",
    "    eid = np.array(edge_data[edge_data['friend_level'] == e].index)\n",
    "    u = th.Tensor(all_triplets[eid, 0])\n",
    "    v = th.Tensor(all_triplets[eid, 2])\n",
    "    _, inverse_index, count = th.unique(v, return_inverse=True, return_counts=True)\n",
    "    degrees = count[inverse_index]\n",
    "     \n",
    "    norm = th.ones(eid.shape[0]) / degrees\n",
    "    norm = norm.unsqueeze(1)\n",
    "    edge_norm[eid] = np.array(norm).reshape(-1)\n",
    " \n",
    " \n",
    "g = dgl.graph((all_triplets[:,0], all_triplets[:,2]), num_nodes = 299962) # 前面建图的地方有写具体含义\n",
    " \n",
    "g.ndata['_TYPE'] = th.Tensor([0]*299962).long()\n",
    "g.edata['_TYPE'] = th.Tensor(all_triplets[:,1]).long()\n",
    "g.ndata['_ID'] = th.Tensor(np.array(range(299962))).long()\n",
    "g.edata['_ID'] = th.Tensor(np.array(range(len(all_triplets)))).long()\n",
    "g.edata['norm'] = th.Tensor(edge_norm.reshape(edge_norm.shape[0],1))\n",
    " \n",
    "category_id = 0 # len(g.ntypes) = 1\n",
    "node_ids = th.arange(g.number_of_nodes())\n",
    "node_tids = g.ndata[dgl.NTYPE]\n",
    "loc = (node_tids == category_id)\n",
    "target_idx = node_ids[loc]\n",
    "target_idx.share_memory_()\n",
    " \n",
    "node_feats = []\n",
    "node_feats.append(th.Tensor(feature).share_memory_())\n",
    " \n",
    " \n",
    "# R-GCN函数\n",
    " \n",
    " \n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.multiprocessing import Queue\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from functools import partial\n",
    "from dgl.nn import RelGraphConv\n",
    "from _thread import start_new_thread\n",
    " \n",
    "def compute_acc(pred, labels): # 计算准确率\n",
    "    y_pred = th.argmax(pred, dim=1).cpu() # 按行取argmax,得到预测的标签\n",
    "    labels = labels.cpu()\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, y_pred).ravel() # y_true, y_pred\n",
    "     \n",
    "    accuracy = (tn+tp)/len(labels)\n",
    "    pos_acc = tp/sum(labels).item()\n",
    "    neg_acc = tn/(len(y_pred)-sum(y_pred).item()) # [y_true=0 & y_pred=0] / y_pred=0\n",
    "     \n",
    "    neg_recall = tn / (tn+fp) # [y_true=0 & y_pred=0] / y_true=0\n",
    "    return neg_recall, neg_acc, pos_acc, accuracy\n",
    " \n",
    "class BaseRGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False, use_cuda=False):\n",
    "        super(BaseRGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.use_cuda = use_cuda\n",
    " \n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    " \n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # h2o\n",
    "        h2o = self.build_output_layer()\n",
    "        if h2o is not None:\n",
    "            self.layers.append(h2o)\n",
    " \n",
    "    def build_input_layer(self):\n",
    "        return None\n",
    " \n",
    "    def build_hidden_layer(self, idx):\n",
    "        raise NotImplementedError\n",
    " \n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    " \n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h\n",
    " \n",
    "class RelGraphEmbedLayer(nn.Module):\n",
    "    r\"\"\"Embedding layer for featureless heterograph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dev_id : int\n",
    "        Device to run the layer.\n",
    "    num_nodes : int\n",
    "        Number of nodes.\n",
    "    node_tides : tensor\n",
    "        Storing the node type id for each node starting from 0\n",
    "    num_of_ntype : int\n",
    "        Number of node types\n",
    "    input_size : list of int\n",
    "        A list of input feature size for each node type. If None, we then\n",
    "        treat certain input feature as an one-hot encoding feature.\n",
    "    embed_size : int\n",
    "        Output embed size\n",
    "    embed_name : str, optional\n",
    "        Embed name\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dev_id,\n",
    "                 num_nodes,\n",
    "                 node_tids,\n",
    "                 num_of_ntype,\n",
    "                 input_size,\n",
    "                 embed_size,\n",
    "                 sparse_emb=False,\n",
    "                 embed_name='embed'):\n",
    "        super(RelGraphEmbedLayer, self).__init__()\n",
    "        self.dev_id = dev_id\n",
    "        self.embed_size = embed_size\n",
    "        self.embed_name = embed_name\n",
    "        self.num_nodes = num_nodes\n",
    "        self.sparse_emb = sparse_emb\n",
    " \n",
    "        # create weight embeddings for each node for each relation\n",
    "        self.embeds = nn.ParameterDict()\n",
    "        self.num_of_ntype = num_of_ntype\n",
    "        self.idmap = th.empty(num_nodes).long()\n",
    " \n",
    "        for ntype in range(num_of_ntype):\n",
    "            if input_size[ntype] is not None:\n",
    "                input_emb_size = input_size[ntype].shape[1]\n",
    "                embed = nn.Parameter(th.Tensor(input_emb_size, self.embed_size))\n",
    "                nn.init.xavier_uniform_(embed)\n",
    "                self.embeds[str(ntype)] = embed\n",
    " \n",
    "        self.node_embeds = th.nn.Embedding(node_tids.shape[0], self.embed_size, sparse=self.sparse_emb)\n",
    "        nn.init.uniform_(self.node_embeds.weight, -1.0, 1.0)\n",
    " \n",
    "    def forward(self, node_ids, node_tids, type_ids, features):\n",
    "        \"\"\"Forward computation\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_ids : tensor\n",
    "            node ids to generate embedding for.\n",
    "        node_ids : tensor\n",
    "            node type ids\n",
    "        features : list of features\n",
    "            list of initial features for nodes belong to different node type.\n",
    "            If None, the corresponding features is an one-hot encoding feature,\n",
    "            else use the features directly as input feature and matmul a\n",
    "            projection matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            embeddings as the input of the next layer\n",
    "        \"\"\"\n",
    "        tsd_ids = node_ids.to(self.node_embeds.weight.device)\n",
    "        embeds = th.empty(node_ids.shape[0], self.embed_size, device=self.dev_id)\n",
    "        for ntype in range(self.num_of_ntype):\n",
    "            if features[ntype] is not None:\n",
    "                loc = node_tids == ntype\n",
    "                embeds[loc] = features[ntype][type_ids[loc]].to(self.dev_id) @ self.embeds[str(ntype)].to(self.dev_id)\n",
    "            else:\n",
    "                loc = node_tids == ntype\n",
    "                embeds[loc] = self.node_embeds(tsd_ids[loc]).to(self.dev_id)\n",
    " \n",
    "        return embeds\n",
    "class EntityClassify(nn.Module):\n",
    "    \"\"\" Entity classification class for RGCN\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : int\n",
    "        Device to run the layer.\n",
    "    num_nodes : int\n",
    "        Number of nodes.\n",
    "    h_dim : int\n",
    "        Hidden dim size.\n",
    "    out_dim : int\n",
    "        Output dim size.\n",
    "    num_rels : int\n",
    "        Numer of relation types.\n",
    "    num_bases : int\n",
    "        Number of bases. If is none, use number of relations.\n",
    "    num_hidden_layers : int\n",
    "        Number of hidden RelGraphConv Layer\n",
    "    dropout : float\n",
    "        Dropout\n",
    "    use_self_loop : bool\n",
    "        Use self loop if True, default False.\n",
    "    low_mem : bool\n",
    "        True to use low memory implementation of relation message passing function\n",
    "        trade speed with memory consumption\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 num_nodes,\n",
    "                 h_dim,\n",
    "                 out_dim,\n",
    "                 num_rels,\n",
    "                 num_bases=None,\n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0,\n",
    "                 use_self_loop=False,\n",
    "                 low_mem=False,\n",
    "                 layer_norm=False):\n",
    "        super(EntityClassify, self).__init__()\n",
    "        self.device = th.device(device if device >= 0 else 'cpu')\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.low_mem = low_mem\n",
    "        self.layer_norm = layer_norm\n",
    " \n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        self.layers.append(RelGraphConv(\n",
    "            self.h_dim, self.h_dim, self.num_rels, \"basis\",\n",
    "            self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "            low_mem=self.low_mem, dropout=self.dropout))\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            self.layers.append(RelGraphConv(\n",
    "                self.h_dim, self.h_dim, self.num_rels, \"basis\",\n",
    "                self.num_bases, activation=F.relu, self_loop=self.use_self_loop,\n",
    "                low_mem=self.low_mem, dropout=self.dropout))\n",
    "        # h2o\n",
    "        self.layers.append(RelGraphConv(\n",
    "            self.h_dim, self.out_dim, self.num_rels, \"basis\",\n",
    "            self.num_bases, activation=None,\n",
    "            self_loop=self.use_self_loop,\n",
    "            low_mem=self.low_mem))\n",
    " \n",
    "    def forward(self, blocks, feats, norm=None):\n",
    "        if blocks is None:\n",
    "            # full graph training\n",
    "            blocks = [self.g] * len(self.layers)\n",
    "        h = feats\n",
    "#         print('1: ', h.shape)\n",
    "        for layer, block in zip(self.layers, blocks):\n",
    "            block = block.to(self.device)\n",
    "#             print('2: ', h.shape)\n",
    "            h = layer(block, h, block.edata['etype'], block.edata['norm'])\n",
    "#             print('3: ', h.shape)\n",
    "        return h\n",
    " \n",
    "class NeighborSampler:\n",
    "    \"\"\"Neighbor sampler\n",
    "    Parameters\n",
    "    ----------\n",
    "    g : DGLHeterograph\n",
    "        Full graph\n",
    "    target_idx : tensor\n",
    "        The target training node IDs in g\n",
    "    fanouts : list of int\n",
    "        Fanout of each hop starting from the seed nodes. If a fanout is None,\n",
    "        sample full neighbors.\n",
    "    \"\"\"\n",
    "    def __init__(self, g, target_idx, fanouts):\n",
    "        self.g = g\n",
    "        self.target_idx = target_idx\n",
    "        self.fanouts = fanouts\n",
    " \n",
    "    \"\"\"Do neighbor sample\n",
    "    Parameters\n",
    "    ----------\n",
    "    seeds :\n",
    "        Seed nodes\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        Seed nodes, also known as target nodes\n",
    "    blocks\n",
    "        Sampled subgraphs\n",
    "    \"\"\"\n",
    "    def sample_blocks(self, seeds):\n",
    "        blocks = []\n",
    "        etypes = []\n",
    "        norms = []\n",
    "        ntypes = []\n",
    "        seeds = th.tensor(seeds).long()\n",
    "        cur = self.target_idx[seeds]\n",
    "        for fanout in self.fanouts:\n",
    "            if fanout is None or fanout == -1:\n",
    "                frontier = dgl.in_subgraph(self.g, cur)\n",
    "            else:\n",
    "                frontier = dgl.sampling.sample_neighbors(self.g, cur, fanout)\n",
    "            etypes = self.g.edata[dgl.ETYPE][frontier.edata[dgl.EID]]\n",
    "            norm = self.g.edata['norm'][frontier.edata[dgl.EID]]\n",
    "            block = dgl.to_block(frontier, cur)\n",
    "            block.srcdata[dgl.NTYPE] = self.g.ndata[dgl.NTYPE][block.srcdata[dgl.NID]]\n",
    "            block.srcdata['type_id'] =self.g.ndata[dgl.NID][block.srcdata[dgl.NID]]\n",
    "            block.edata['etype'] = etypes\n",
    "            block.edata['norm'] = norm\n",
    "            cur = block.srcdata[dgl.NID]\n",
    "            blocks.insert(0, block)\n",
    "        return seeds, blocks\n",
    " \n",
    "# https://github.com/classicsong/dgl/blob/a5d10b893877bf58dd9322804b8a552ffdbaf932/examples/pytorch/rgcn/utils.py\n",
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    " \n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    " \n",
    "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges by neighborhool expansion.\n",
    "    This guarantees that the sampled edges form a connected graph, which\n",
    "    may help deeper GNNs that require information from more than one hop.\n",
    "    \"\"\"\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    " \n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in range(n_triplets)])\n",
    "    seen = np.array([False for _ in degrees])\n",
    " \n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    " \n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    " \n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
    "                                         p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    " \n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    " \n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    " \n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    " \n",
    "    return edges\n",
    " \n",
    "def sample_edge_uniform(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triplets)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    " \n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
    "                                      num_rels, adj_list, degrees,\n",
    "                                      negative_rate, sampler=\"uniform\"):\n",
    "    \"\"\"Get training graph and signals\n",
    "    First perform edge neighborhood sampling on graph, then perform negative\n",
    "    sampling to generate negative samples\n",
    "    \"\"\"\n",
    "    # perform edge neighbor sampling\n",
    "    if sampler == \"uniform\":\n",
    "        edges = sample_edge_uniform(adj_list, degrees, len(triplets), sample_size)\n",
    "    elif sampler == \"neighbor\":\n",
    "        edges = sample_edge_neighborhood(adj_list, degrees, len(triplets), sample_size)\n",
    "    else:\n",
    "        raise ValueError(\"Sampler type must be either 'uniform' or 'neighbor'.\")\n",
    " \n",
    "    # relabel nodes to have consecutive node ids\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    " \n",
    "    # negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
    "                                        negative_rate)\n",
    " \n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "    src = src[graph_split_ids]\n",
    "    dst = dst[graph_split_ids]\n",
    "    rel = rel[graph_split_ids]\n",
    " \n",
    "    # build DGL graph\n",
    "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
    "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
    "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
    "                                             (src, rel, dst))\n",
    "    return g, uniq_v, rel, norm, samples, labels\n",
    " \n",
    "def comp_deg_norm(g):\n",
    "    g = g.local_var()\n",
    "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
    "    norm = 1.0 / in_deg\n",
    "    norm[np.isinf(norm)] = 0\n",
    "    return norm\n",
    " \n",
    "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
    "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
    "        use reversed relations.\n",
    "        This function also generates edge type and normalization factor\n",
    "        (reciprocal of node incoming degree)\n",
    "    \"\"\"\n",
    "    g = dgl.DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    src, rel, dst = triplets\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    rel = np.concatenate((rel, rel + num_rels))\n",
    "    edges = sorted(zip(dst, src, rel))\n",
    "    dst, src, rel = np.array(edges).transpose()\n",
    "    g.add_edges(src, dst)\n",
    "    norm = comp_deg_norm(g)\n",
    "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
    "    return g, rel.astype('int64'), norm.astype('int64')\n",
    " \n",
    "def build_test_graph(num_nodes, num_rels, edges):\n",
    "    src, rel, dst = edges.transpose()\n",
    "    print(\"Test graph:\")\n",
    "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
    " \n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.randint(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    " \n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    " \n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (raw)\n",
    "#\n",
    "#######################################################################\n",
    " \n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    " \n",
    "def perturb_and_get_raw_rank(embedding, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\n",
    "    \"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = min(test_size, (idx + 1) * batch_size)\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = embedding[batch_a] * w[batch_r]\n",
    "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
    "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
    "        score = torch.sum(out_prod, dim=0) # size E x V\n",
    "        score = torch.sigmoid(score)\n",
    "        target = b[batch_start: batch_end]\n",
    "        ranks.append(sort_and_rank(score, target))\n",
    "    return torch.cat(ranks)\n",
    " \n",
    "# return MRR (raw), and Hits @ (1, 3, 10)\n",
    "def calc_raw_mrr(embedding, w, test_triplets, hits=[], eval_bz=100):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    " \n",
    "        # perturb subject\n",
    "        ranks_s = perturb_and_get_raw_rank(embedding, w, o, r, s, test_size, eval_bz)\n",
    "        # perturb object\n",
    "        ranks_o = perturb_and_get_raw_rank(embedding, w, s, r, o, test_size, eval_bz)\n",
    " \n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    " \n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
    " \n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    " \n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (filtered)\n",
    "#\n",
    "#######################################################################\n",
    " \n",
    "def filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_o = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider an object if it is part of a triplet to filter\n",
    "    for o in range(num_entities):\n",
    "        if (target_s, target_r, o) not in triplets_to_filter:\n",
    "            filtered_o.append(o)\n",
    "    return torch.LongTensor(filtered_o)\n",
    " \n",
    "def filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_s = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider a subject if it is part of a triplet to filter\n",
    "    for s in range(num_entities):\n",
    "        if (s, target_r, target_o) not in triplets_to_filter:\n",
    "            filtered_s.append(s)\n",
    "    return torch.LongTensor(filtered_s)\n",
    " \n",
    "def perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb object in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_o = filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_o_idx = int((filtered_o == target_o).nonzero())\n",
    "        emb_s = embedding[target_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[filtered_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_o_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    " \n",
    "def perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb subject in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_s = filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_s_idx = int((filtered_s == target_s).nonzero())\n",
    "        emb_s = embedding[filtered_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[target_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_s_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    " \n",
    "def calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    " \n",
    "        triplets_to_filter = torch.cat([train_triplets, valid_triplets, test_triplets]).tolist()\n",
    "        triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter}\n",
    "        print('Perturbing subject...')\n",
    "        ranks_s = perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "        print('Perturbing object...')\n",
    "        ranks_o = perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    " \n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    " \n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    " \n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    " \n",
    "#######################################################################\n",
    "#\n",
    "# Main evaluation function\n",
    "#\n",
    "#######################################################################\n",
    " \n",
    "def calc_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[], eval_bz=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits)\n",
    "    else:\n",
    "        mrr = calc_raw_mrr(embedding, w, test_triplets, hits, eval_bz)\n",
    "    return mrr\n",
    " \n",
    " \n",
    "#######################################################################\n",
    "#\n",
    "# Multithread wrapper\n",
    "#\n",
    "#######################################################################\n",
    " \n",
    "# According to https://github.com/pytorch/pytorch/issues/17199, this decorator\n",
    "# is necessary to make fork() and openmp work together.\n",
    "def thread_wrapped_func(func):\n",
    "    \"\"\"\n",
    "    Wraps a process entry point to make it work with OpenMP.\n",
    "    \"\"\"\n",
    "    from functools import wraps\n",
    "    @wraps(func)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        queue = Queue()\n",
    "        def _queue_result():\n",
    "            exception, trace, res = None, None, None\n",
    "            try:\n",
    "                res = func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                exception = e\n",
    "                trace = traceback.format_exc()\n",
    "            queue.put((res, exception, trace))\n",
    " \n",
    "        start_new_thread(_queue_result, ())\n",
    "        result, exception, trace = queue.get()\n",
    "        if exception is None:\n",
    "            return result\n",
    "        else:\n",
    "            assert isinstance(exception, Exception)\n",
    "            raise exception.__class__(trace)\n",
    "    return decorated_function\n",
    "@thread_wrapped_func\n",
    "def run(proc_id, n_gpus, args, devices, dataset, split, queue=None):\n",
    "    dev_id = devices[proc_id]\n",
    "    g, node_feats, num_of_ntype, num_classes, num_rels, target_idx, \\\n",
    "        train_idx, val_idx, test_idx, labels = dataset\n",
    "    if split is not None:\n",
    "        train_seed, val_seed, test_seed = split\n",
    "        train_idx = train_idx[train_seed]\n",
    "        val_idx = val_idx[val_seed]\n",
    "        test_idx = test_idx[test_seed]\n",
    " \n",
    "    fanouts = [int(fanout) for fanout in args.fanout.split(',')]\n",
    "    node_tids = g.ndata[dgl.NTYPE]\n",
    "    sampler = NeighborSampler(g, target_idx, fanouts)\n",
    "    loader = DataLoader(dataset=train_idx.numpy(),\n",
    "                        batch_size=args.batch_size,\n",
    "                        collate_fn=sampler.sample_blocks,\n",
    "                        shuffle=True,\n",
    "                        num_workers=args.num_workers)\n",
    " \n",
    "    # validation sampler\n",
    "    val_sampler = NeighborSampler(g, target_idx, [None] * args.n_layers)\n",
    "    val_loader = DataLoader(dataset=val_idx.numpy(),\n",
    "                            batch_size=args.eval_batch_size,\n",
    "                            collate_fn=val_sampler.sample_blocks,\n",
    "                            shuffle=False,\n",
    "                            num_workers=args.num_workers)\n",
    " \n",
    "    # validation sampler\n",
    "    test_sampler = NeighborSampler(g, target_idx, [None] * args.n_layers)\n",
    "    test_loader = DataLoader(dataset=test_idx.numpy(),\n",
    "                             batch_size=args.eval_batch_size,\n",
    "                             collate_fn=test_sampler.sample_blocks,\n",
    "                             shuffle=False,\n",
    "                             num_workers=args.num_workers)\n",
    " \n",
    "    if n_gpus > 1:\n",
    "        dist_init_method = 'tcp://{master_ip}:{master_port}'.format(\n",
    "            master_ip='127.0.0.1', master_port='12345')\n",
    "        world_size = n_gpus\n",
    "        backend = 'nccl'\n",
    " \n",
    "        # using sparse embedding or usig mix_cpu_gpu model (embedding model can not be stored in GPU)\n",
    "        if args.sparse_embedding or args.mix_cpu_gpu:\n",
    "            backend = 'gloo'\n",
    "        th.distributed.init_process_group(backend=backend,\n",
    "                                          init_method=dist_init_method,\n",
    "                                          world_size=world_size,\n",
    "                                          rank=dev_id)\n",
    " \n",
    "    # node features\n",
    "    # None for one-hot feature, if not none, it should be the feature tensor.\n",
    "    # \n",
    "    embed_layer = RelGraphEmbedLayer(dev_id,\n",
    "                                     g.number_of_nodes(),\n",
    "                                     node_tids,\n",
    "                                     num_of_ntype,\n",
    "                                     node_feats,\n",
    "                                     args.n_hidden,\n",
    "                                     sparse_emb=args.sparse_embedding)\n",
    " \n",
    "    # create model\n",
    "    # all model params are in device.\n",
    "    model = EntityClassify(dev_id,\n",
    "                           g.number_of_nodes(),\n",
    "                           args.n_hidden,\n",
    "                           num_classes,\n",
    "                           num_rels,\n",
    "                           num_bases=args.n_bases,\n",
    "                           num_hidden_layers=args.n_layers - 2,\n",
    "                           dropout=args.dropout,\n",
    "                           use_self_loop=args.use_self_loop,\n",
    "                           low_mem=args.low_mem,\n",
    "                           layer_norm=args.layer_norm)\n",
    " \n",
    "    if dev_id >= 0 and n_gpus == 1:\n",
    "        th.cuda.set_device(dev_id)\n",
    "        labels = labels.to(dev_id)\n",
    "        model.cuda(dev_id)\n",
    "        # embedding layer may not fit into GPU, then use mix_cpu_gpu\n",
    "        if args.mix_cpu_gpu is False:\n",
    "            embed_layer.cuda(dev_id)\n",
    " \n",
    "    if n_gpus > 1:\n",
    "        labels = labels.to(dev_id)\n",
    "        model.cuda(dev_id)\n",
    "        if args.mix_cpu_gpu:\n",
    "            embed_layer = DistributedDataParallel(embed_layer, device_ids=None, output_device=None)\n",
    "        else:\n",
    "            embed_layer.cuda(dev_id)\n",
    "            embed_layer = DistributedDataParallel(embed_layer, device_ids=[dev_id], output_device=dev_id)\n",
    "        model = DistributedDataParallel(model, device_ids=[dev_id], output_device=dev_id)\n",
    " \n",
    "    # optimizer\n",
    "    if args.sparse_embedding:\n",
    "        dense_params = list(model.parameters())\n",
    "        if args.node_feats:\n",
    "            if  n_gpus > 1:\n",
    "                dense_params += list(embed_layer.module.embeds.parameters())\n",
    "            else:\n",
    "                dense_params += list(embed_layer.embeds.parameters())\n",
    "        optimizer = th.optim.Adam(dense_params, lr=args.lr, weight_decay=args.l2norm)\n",
    "        if  n_gpus > 1:\n",
    "            emb_optimizer = th.optim.SparseAdam(embed_layer.module.node_embeds.parameters(), lr=args.lr)\n",
    "        else:\n",
    "            emb_optimizer = th.optim.SparseAdam(embed_layer.node_embeds.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        all_params = list(model.parameters()) + list(embed_layer.parameters())\n",
    "        optimizer = th.optim.Adam(all_params, lr=args.lr, weight_decay=args.l2norm)\n",
    " \n",
    "    # training loop\n",
    "    print(\"start training...\")\n",
    "    forward_time = []\n",
    "    backward_time = []\n",
    " \n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "         \n",
    "        for i, sample_data in enumerate(loader):\n",
    "            seeds, blocks = sample_data\n",
    "            t0 = time.time()\n",
    "            if args.mix_cpu_gpu is False:\n",
    "                feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "                                    blocks[0].srcdata[dgl.NTYPE],\n",
    "                                    blocks[0].srcdata['type_id'],\n",
    "                                    node_feats)\n",
    "            else:\n",
    "                feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "                                    blocks[0].srcdata[dgl.NTYPE],\n",
    "                                    blocks[0].srcdata['type_id'],\n",
    "                                    node_feats)\n",
    "            logits = model(blocks, feats)\n",
    "            loss = F.cross_entropy(logits, labels[seeds])\n",
    "            t1 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            if args.sparse_embedding:\n",
    "                emb_optimizer.zero_grad()\n",
    " \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if args.sparse_embedding:\n",
    "                emb_optimizer.step()\n",
    "            t2 = time.time()\n",
    " \n",
    "            forward_time.append(t1 - t0)\n",
    "            backward_time.append(t2 - t1)\n",
    "            train_neg_recall, train_neg_acc, train_pos_acc, train_accuracy = compute_acc(logits, labels[seeds])\n",
    "            print('Epoch {} |Sample = {}/{} |Neg Recall = {:.2f} |Neg Acc = {:.2f} |Pos Acc = {:.2f} |All Acc = {:.2f} |Loss = {:.2f}'.format(\n",
    "                epoch, i+1, len(loader), train_neg_recall, train_neg_acc, train_pos_acc, train_accuracy, loss.item()))\n",
    " \n",
    "        print(\"Epoch {:05d}:{:05d} | Train Forward Time(s) {:.4f} | Backward Time(s) {:.4f}\".\n",
    "            format(epoch, i+1, forward_time[-1], backward_time[-1]))\n",
    "        th.save(model.state_dict(), './checkpoints/model_5.3_RGCN_nodefeature_mp.pt')\n",
    "        th.save(model, './checkpoints/model_5.3_RGCN_nodefeature_mp.pkl')\n",
    "#         # only process 0 will do the evaluation\n",
    "#         if (queue is not None) or (proc_id == 0):\n",
    "#             model.eval()\n",
    "#             eval_logits = []\n",
    "#             eval_seeds = []\n",
    "#             with th.no_grad():\n",
    "#                 for sample_data in tqdm(val_loader):\n",
    "#                     th.cuda.empty_cache()\n",
    "#                     seeds, blocks = sample_data\n",
    "#                     if args.mix_cpu_gpu is False:\n",
    "#                         feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "#                                             blocks[0].srcdata[dgl.NTYPE],\n",
    "#                                             blocks[0].srcdata['type_id'],\n",
    "#                                             node_feats)\n",
    "#                     else:\n",
    "#                         feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "#                                             blocks[0].srcdata[dgl.NTYPE],\n",
    "#                                             blocks[0].srcdata['type_id'],\n",
    "#                                             node_feats)\n",
    "#                     logits = model(blocks, feats)\n",
    "#                     eval_logits.append(logits.cpu().detach())\n",
    "#                     eval_seeds.append(seeds.cpu().detach())\n",
    "#                 eval_logits = th.cat(eval_logits)\n",
    "#                 eval_seeds = th.cat(eval_seeds)\n",
    "#                 if queue is not None:\n",
    "#                     queue.put((eval_logits, eval_seeds))\n",
    " \n",
    "#             if proc_id == 0:\n",
    "#                 if queue is not None:\n",
    "#                     eval_logits = []\n",
    "#                     eval_seeds = []\n",
    "#                     for i in range(n_gpus):\n",
    "#                         log = queue.get()\n",
    "#                         val_l, val_s = log\n",
    "#                         eval_logits.append(val_l)\n",
    "#                         eval_seeds.append(val_s)\n",
    "#                     eval_logits = th.cat(eval_logits)\n",
    "#                     eval_seeds = th.cat(eval_seeds)\n",
    "#                 val_loss = F.cross_entropy(eval_logits, labels[eval_seeds].cpu()).item()\n",
    "#                 val_acc = th.sum(eval_logits.argmax(dim=1) == labels[eval_seeds].cpu()).item() / len(eval_seeds)\n",
    " \n",
    "#                 print(\"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".\n",
    "#                         format(val_acc, val_loss))\n",
    "        if n_gpus > 1:\n",
    "            th.distributed.barrier()\n",
    "    print('===========Test start==========')\n",
    " \n",
    "    # only process 0 will do the evaluation\n",
    "    if (queue is not None) or (proc_id == 0):\n",
    "        model.eval()\n",
    "        test_logits = []\n",
    "        test_seeds = []\n",
    "        with th.no_grad():\n",
    "            for sample_data in tqdm(test_loader):\n",
    "                th.cuda.empty_cache()\n",
    "                seeds, blocks = sample_data\n",
    "                if args.mix_cpu_gpu is False:\n",
    "                    feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "                                        blocks[0].srcdata[dgl.NTYPE],\n",
    "                                        blocks[0].srcdata['type_id'],\n",
    "                                        node_feats)\n",
    "                else:\n",
    "                    feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "                                        blocks[0].srcdata[dgl.NTYPE],\n",
    "                                        blocks[0].srcdata['type_id'],\n",
    "                                        node_feats)\n",
    "                logits = model(blocks, feats)\n",
    "                test_logits.append(logits.cpu().detach())\n",
    "                test_seeds.append(seeds.cpu().detach())\n",
    "            test_logits = th.cat(test_logits)\n",
    "            test_seeds = th.cat(test_seeds)\n",
    " \n",
    "            if queue is not None:\n",
    "                queue.put((test_logits, test_seeds))\n",
    " \n",
    "        if proc_id == 0:\n",
    "            if queue is not None:\n",
    "                test_logits = []\n",
    "                test_seeds = []\n",
    "                for i in range(n_gpus):\n",
    "                    log = queue.get()\n",
    "                    test_l, test_s = log\n",
    "                    test_logits.append(test_l)\n",
    "                    test_seeds.append(test_s)\n",
    "                test_logits = th.cat(test_logits)\n",
    "                test_seeds = th.cat(test_seeds)\n",
    "            test_loss = F.cross_entropy(test_logits, labels[test_seeds].cpu()).item()\n",
    "            test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy = compute_acc(test_logits, labels[test_seeds])\n",
    "            print('Test: Neg Recall = {:.2f} | Neg Acc = {:.2f} | Pos Acc = {:.2f} | All Acc = {:.2f} | Loss = {:.4f}'.format(\n",
    "                test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy, test_loss))\n",
    "            print()\n",
    " \n",
    "    # sync for test\n",
    "    if n_gpus > 1:\n",
    "        th.distributed.barrier()\n",
    " \n",
    "    print(\"{}/{} Mean forward time: {:4f}\".format(proc_id, n_gpus,\n",
    "                                                  np.mean(forward_time[len(forward_time) // 4:])))\n",
    "    print(\"{}/{} Mean backward time: {:4f}\".format(proc_id, n_gpus,\n",
    "                                                   np.mean(backward_time[len(backward_time) // 4:])))\n",
    " \n",
    " \n",
    "# 参数设置\n",
    " \n",
    " \n",
    "parser = argparse.ArgumentParser(description='RGCN')\n",
    "parser.add_argument(\"--dropout\", type=float, default=0,\n",
    "        help=\"dropout probability\")\n",
    "parser.add_argument(\"--n-hidden\", type=int, default=64,\n",
    "        help=\"number of hidden units\")\n",
    "parser.add_argument(\"--gpu\", type=str, default='0',\n",
    "        help=\"gpu\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "        help=\"learning rate\")\n",
    "parser.add_argument(\"--n-bases\", type=int, default=-1,\n",
    "        help=\"number of filter weight matrices, default: -1 [use all]\")\n",
    "parser.add_argument(\"--n-layers\", type=int, default=2,\n",
    "        help=\"number of propagation rounds\")\n",
    "parser.add_argument(\"-e\", \"--n-epochs\", type=int, default=5,\n",
    "        help=\"number of training epochs\")\n",
    "parser.add_argument(\"--l2norm\", type=float, default=0,\n",
    "        help=\"l2 norm coef\")\n",
    "parser.add_argument(\"--relabel\", default=False, action='store_true',\n",
    "        help=\"remove untouched nodes and relabel\")\n",
    "parser.add_argument(\"--fanout\", type=str, default=\"5, 5\",\n",
    "        help=\"Fan-out of neighbor sampling.\")\n",
    "parser.add_argument(\"--use-self-loop\", default=False, action='store_true',\n",
    "        help=\"include self feature as a special relation\")\n",
    "fp = parser.add_mutually_exclusive_group(required=False)\n",
    "fp.add_argument('--validation', dest='validation', action='store_true')\n",
    "fp.add_argument('--testing', dest='validation', action='store_false')\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. \")\n",
    "parser.add_argument(\"--eval-batch-size\", type=int, default=1024,\n",
    "        help=\"Mini-batch size. \")\n",
    "parser.add_argument(\"--num-workers\", type=int, default=0,\n",
    "        help=\"Number of workers for dataloader.\")\n",
    "parser.add_argument(\"--low-mem\", default=False, action='store_true',\n",
    "        help=\"Whether use low mem RelGraphCov\")\n",
    "parser.add_argument(\"--mix-cpu-gpu\", default=False, action='store_true',\n",
    "        help=\"Whether store node embeddins in cpu\")\n",
    "parser.add_argument(\"--sparse-embedding\", action='store_true',\n",
    "        help='Use sparse embedding for node embeddings.')\n",
    "parser.add_argument('--node-feats', default=True, action='store_true',\n",
    "        help='Whether use node features')\n",
    "parser.add_argument('--global-norm', default=False, action='store_true',\n",
    "        help='User global norm instead of per node type norm')\n",
    "parser.add_argument('--layer-norm', default=False, action='store_true',\n",
    "        help='Use layer norm')\n",
    "parser.set_defaults(validation=True)\n",
    "args = parser.parse_args(args = [])\n",
    " \n",
    "num_of_ntype = len(g.ntypes) # 1\n",
    "num_rels = 149 # g.canonical_etypes = 1\n",
    "num_classes = 2\n",
    " \n",
    "train_idx = th.Tensor(train_idx).long()\n",
    "val_idx = train_idx\n",
    "test_idx = th.Tensor(test_idx).long()\n",
    "labels = th.Tensor(labels).long()\n",
    " \n",
    " \n",
    "# 训练和测试\n",
    " \n",
    " \n",
    "args.gpu = '0'\n",
    "devices = list(map(int, args.gpu.split(',')))\n",
    "n_gpus = len(devices)\n",
    " \n",
    "# cpu\n",
    "if devices[0] == -1: \n",
    "    run(0, 0, args, ['cpu'], \n",
    "        (g, num_of_ntype, num_classes, num_rels, target_idx,\n",
    "         train_idx, val_idx, test_idx, labels))\n",
    "# gpu\n",
    "elif n_gpus == 1:\n",
    "    run(0, n_gpus, args, devices,\n",
    "        (g, node_feats, num_of_ntype, num_classes, num_rels, target_idx,\n",
    "        train_idx, val_idx, test_idx, labels), None, None)\n",
    "# multi gpu\n",
    "else:\n",
    "    procs = []\n",
    "    num_train_seeds = train_idx.shape[0]\n",
    "    tseeds_per_proc = num_train_seeds // n_gpus\n",
    "    for proc_id in range(n_gpus):\n",
    "        proc_train_seeds = train_idx[proc_id * tseeds_per_proc :\n",
    "                                     (proc_id + 1) * tseeds_per_proc \\\n",
    "                                     if (proc_id + 1) * tseeds_per_proc < num_train_seeds \\\n",
    "                                     else num_train_seeds]\n",
    "        p = mp.Process(target=run, args=(proc_id, n_gpus, args, devices,\n",
    "                                         (g, num_of_ntype, num_classes, num_rels, target_idx,\n",
    "                                         proc_train_seeds, val_idx, test_idx, labels)))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()\n",
    " \n",
    "# 加载模型测试\n",
    " \n",
    " \n",
    "model_ = th.load('./checkpoints/model_5.3_RGCN_nodefeature_mp.pkl')\n",
    " \n",
    "dev_id = 0\n",
    "queue=None\n",
    "embed_layer = RelGraphEmbedLayer(dev_id,\n",
    "                                     g.number_of_nodes(),\n",
    "                                     node_tids,\n",
    "                                     num_of_ntype,\n",
    "                                     node_feats,\n",
    "                                     args.n_hidden,\n",
    "                                     sparse_emb=args.sparse_embedding)\n",
    " \n",
    "test_sampler = NeighborSampler(g, target_idx, [None] * args.n_layers)\n",
    "test_loader = DataLoader(dataset=test_idx.numpy(),\n",
    "                             batch_size=args.eval_batch_size,\n",
    "                             collate_fn=test_sampler.sample_blocks,\n",
    "                             shuffle=False,\n",
    "                             num_workers=args.num_workers)\n",
    "model_.eval()\n",
    "test_logits = []\n",
    "test_seeds = []\n",
    "with th.no_grad():\n",
    "    for sample_data in tqdm(test_loader):\n",
    "        th.cuda.empty_cache()\n",
    "        seeds, blocks = sample_data\n",
    "        if args.mix_cpu_gpu is False:\n",
    "            feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "                                blocks[0].srcdata[dgl.NTYPE],\n",
    "                                blocks[0].srcdata['type_id'],\n",
    "                                node_feats)\n",
    "        else:\n",
    "            feats = embed_layer(blocks[0].srcdata[dgl.NID],\n",
    "                                blocks[0].srcdata[dgl.NTYPE],\n",
    "                                blocks[0].srcdata['type_id'],\n",
    "                                node_feats)\n",
    "        logits = model_(blocks, feats)\n",
    "        test_logits.append(logits.cpu().detach())\n",
    "        test_seeds.append(seeds.cpu().detach())\n",
    "    test_logits = th.cat(test_logits)\n",
    "    test_seeds = th.cat(test_seeds)\n",
    " \n",
    "    if queue is not None:\n",
    "        queue.put((test_logits, test_seeds))\n",
    " \n",
    "#     if proc_id == 0:\n",
    "    if queue is not None:\n",
    "        test_logits = []\n",
    "        test_seeds = []\n",
    "        for i in range(n_gpus):\n",
    "            log = queue.get()\n",
    "            test_l, test_s = log\n",
    "            test_logits.append(test_l)\n",
    "            test_seeds.append(test_s)\n",
    "        test_logits = th.cat(test_logits)\n",
    "        test_seeds = th.cat(test_seeds)\n",
    "    test_loss = F.cross_entropy(test_logits, labels[test_seeds].cpu()).item()\n",
    "    test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy = compute_acc(test_logits, labels[test_seeds])\n",
    "    print('Test: Neg Recall = {:.2f} | Neg Acc = {:.2f} | Pos Acc = {:.2f} | All Acc = {:.2f} | Loss = {:.4f}'.format(\n",
    "        test_neg_recall, test_neg_acc, test_pos_acc, test_accuracy, test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
