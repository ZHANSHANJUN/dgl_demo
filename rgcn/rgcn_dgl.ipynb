{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205758ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef328fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNLayer(nn.Module):\n",
    "    # 参数说明：\n",
    "    # in_feat 输入维度\n",
    "    # out_feat 输出维度\n",
    "    # num_rels 边类型数量\n",
    "    # num_bases W_r分解的数量，对应原文公式3的B（求和符号的上界）\n",
    "    # bias 偏置\n",
    "    # activation 激活函数\n",
    "    # is_input_layer 是否是输入层（第一层）\n",
    "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
    "                 activation=None, is_input_layer=False):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        # 矩阵分解的参数校验条件：不能小于0，不能比现有维度大（复杂度会变高，参数反而增加）\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # weight bases in equation (3)\n",
    "        # 这里是根据公式3把W_r算出来，用V_b（weight）表示，共有num_bases个V_b累加得到\n",
    "        # 得到的结果是Tensor，因此用 nn.Parameter将一个不可训练的类型Tensor\n",
    "        # 转换成可以训练的类型parameter\n",
    "        # 并将这个parameter绑定到这个module里面\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
    "                                                self.out_feat))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            # 这里的w_comp是公式3里面的a_{rb}\n",
    "            # 一个边类型对应一个W_r（那么就一共有num_rels种W_r），每个W_r分解为num_bases个组合\n",
    "            # 因此w_comp这里的维度就是num_rels×num_bases\n",
    "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        # add bias\n",
    "        # 偏置要进行加法运算其维度要和输出维度大小一样\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "\n",
    "        # init trainable parameters\n",
    "        # 这里用的是xavier初始化，可以查下为什么\n",
    "        # 因为我们用的是线性激活函数，网上有说用ReLU和Leaky ReLU\n",
    "        # 可以考虑用别的初始化方法：He init 。\n",
    "        nn.init.xavier_uniform_(self.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.bias,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:#分解就走公式3\n",
    "            # generate all weights from bases (equation (3))\n",
    "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
    "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,\n",
    "                                                        self.in_feat, self.out_feat)\n",
    "        else:#不分解就直接用weight算\n",
    "            weight = self.weight\n",
    "\n",
    "        if self.is_input_layer:#如果是第一层\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                # 对于第一层，输入可以直接用独热编码进行aggregate\n",
    "                # 信息的汇聚就可以直接写成矩阵相乘的形式\n",
    "                embed = weight.view(-1, self.out_feat)# embed维度整成out_feat维度一样\n",
    "                index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                return {'msg': embed[index] * edges.data['norm']}\n",
    "        else:\n",
    "            def message_func(edges):\n",
    "                #根据边类型'rel_type'获取对应的w\n",
    "                w = weight[edges.data['rel_type']]\n",
    "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()#消息汇聚，就是w乘以src['h']（输入节点特征）\n",
    "                msg = msg * edges.data['norm']\n",
    "                return {'msg': msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            if self.bias:#有偏置加偏置\n",
    "                h = h + self.bias\n",
    "            if self.activation:#经过激活函数\n",
    "                h = self.activation(h)\n",
    "            return {'h': h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b886bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
    "                 num_bases=-1, num_hidden_layers=1):\n",
    "        # 先初始化参数\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        # 具体看下面函数\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        #torch.arange(start=0, end=5)的结果并不包含end，start默认是0\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    # 输入num_nodes的独热编号\n",
    "    # 输出h_dim\n",
    "    # 激活函数是relu\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(self.num_nodes, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu, is_input_layer=True)\n",
    "\n",
    "    # 输入h_dim\n",
    "    # 输出h_dim\n",
    "    # 激活函数是relu\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
    "                         activation=F.relu)\n",
    "\n",
    "    # 输入h_dim\n",
    "    # 输出out_dim\n",
    "    # 激活函数是softmax后归一化\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(self.h_dim, self.out_dim, self.num_rels, self.num_bases,\n",
    "                         activation=partial(F.softmax, dim=1))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            g.ndata['id'] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop('h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4fc009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/worker/.dgl/aifb.tgz from https://data.dgl.ai/dataset/aifb.tgz...\n",
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "from dgl.contrib.data import load_data\n",
    "data = load_data(dataset='aifb')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]#前20%做验证\n",
    "train_idx = train_idx[len(train_idx) // 5:]#剩下做训练\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20a86776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/lib/python3.9/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "from dgl import DGLGraph, DGLHeteroGraph\n",
    "# configurations\n",
    "n_hidden = 16 # number of hidden units\n",
    "n_bases = -1 # use number of relations as number of bases这里设置-1相当于没进行分解\n",
    "n_hidden_layers = 0 # use 1 input layer, 1 output layer, no hidden layer#输出接输出，没有中间层\n",
    "n_epochs = 25 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient\n",
    "\n",
    "# create graph\n",
    "g = DGLGraph((data.edge_src, data.edge_dst))\n",
    "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "\n",
    "# create model\n",
    "model = Model(g.number_of_nodes(),\n",
    "              n_hidden,\n",
    "              num_classes,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14fec830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.3393 | Train Loss: 1.3859 | Validation Accuracy: 0.2857 | Validation loss: 1.3861\n",
      "Epoch 00001 | Train Accuracy: 0.9643 | Train Loss: 1.3436 | Validation Accuracy: 0.8571 | Validation loss: 1.3594\n",
      "Epoch 00002 | Train Accuracy: 0.9375 | Train Loss: 1.2781 | Validation Accuracy: 0.9643 | Validation loss: 1.3178\n",
      "Epoch 00003 | Train Accuracy: 0.9375 | Train Loss: 1.1969 | Validation Accuracy: 0.9643 | Validation loss: 1.2625\n",
      "Epoch 00004 | Train Accuracy: 0.9375 | Train Loss: 1.1167 | Validation Accuracy: 0.9643 | Validation loss: 1.1992\n",
      "Epoch 00005 | Train Accuracy: 0.9375 | Train Loss: 1.0451 | Validation Accuracy: 0.9643 | Validation loss: 1.1333\n",
      "Epoch 00006 | Train Accuracy: 0.9554 | Train Loss: 0.9813 | Validation Accuracy: 1.0000 | Validation loss: 1.0670\n",
      "Epoch 00007 | Train Accuracy: 0.9554 | Train Loss: 0.9266 | Validation Accuracy: 1.0000 | Validation loss: 1.0031\n",
      "Epoch 00008 | Train Accuracy: 0.9554 | Train Loss: 0.8832 | Validation Accuracy: 1.0000 | Validation loss: 0.9464\n",
      "Epoch 00009 | Train Accuracy: 0.9643 | Train Loss: 0.8508 | Validation Accuracy: 1.0000 | Validation loss: 0.8997\n",
      "Epoch 00010 | Train Accuracy: 0.9643 | Train Loss: 0.8277 | Validation Accuracy: 0.9643 | Validation loss: 0.8639\n",
      "Epoch 00011 | Train Accuracy: 0.9732 | Train Loss: 0.8114 | Validation Accuracy: 0.9643 | Validation loss: 0.8378\n",
      "Epoch 00012 | Train Accuracy: 0.9732 | Train Loss: 0.7997 | Validation Accuracy: 0.9643 | Validation loss: 0.8197\n",
      "Epoch 00013 | Train Accuracy: 0.9732 | Train Loss: 0.7908 | Validation Accuracy: 0.9643 | Validation loss: 0.8075\n",
      "Epoch 00014 | Train Accuracy: 0.9732 | Train Loss: 0.7838 | Validation Accuracy: 0.9643 | Validation loss: 0.7996\n",
      "Epoch 00015 | Train Accuracy: 0.9732 | Train Loss: 0.7780 | Validation Accuracy: 0.9643 | Validation loss: 0.7947\n",
      "Epoch 00016 | Train Accuracy: 0.9821 | Train Loss: 0.7733 | Validation Accuracy: 0.9643 | Validation loss: 0.7919\n",
      "Epoch 00017 | Train Accuracy: 0.9821 | Train Loss: 0.7694 | Validation Accuracy: 0.9643 | Validation loss: 0.7906\n",
      "Epoch 00018 | Train Accuracy: 0.9821 | Train Loss: 0.7662 | Validation Accuracy: 0.9643 | Validation loss: 0.7903\n",
      "Epoch 00019 | Train Accuracy: 0.9821 | Train Loss: 0.7635 | Validation Accuracy: 0.9643 | Validation loss: 0.7909\n",
      "Epoch 00020 | Train Accuracy: 0.9821 | Train Loss: 0.7611 | Validation Accuracy: 0.9643 | Validation loss: 0.7920\n",
      "Epoch 00021 | Train Accuracy: 0.9821 | Train Loss: 0.7589 | Validation Accuracy: 0.9643 | Validation loss: 0.7938\n",
      "Epoch 00022 | Train Accuracy: 0.9821 | Train Loss: 0.7569 | Validation Accuracy: 0.9643 | Validation loss: 0.7960\n",
      "Epoch 00023 | Train Accuracy: 0.9821 | Train Loss: 0.7548 | Validation Accuracy: 0.9643 | Validation loss: 0.7989\n",
      "Epoch 00024 | Train Accuracy: 1.0000 | Train Loss: 0.7527 | Validation Accuracy: 0.9286 | Validation loss: 0.8020\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d71f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
